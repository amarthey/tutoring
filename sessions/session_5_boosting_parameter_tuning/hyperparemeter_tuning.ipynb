{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import data\n",
    "data = pd.read_csv('breast-cancer-wisconsin.csv',header=None)\n",
    "\n",
    "#set column names\n",
    "data.columns = ['Sample Code Number','Clump Thickness','Uniformity of Cell Size',\n",
    "                                                        'Uniformity of Cell Shape','Marginal Adhesion','Single Epithelial Cell Size',\n",
    "                                                        'Bare Nuclei','Bland Chromatin','Normal Nucleoli','Mitoses','Class']\n",
    "#view top 10 rows\n",
    "data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each row in the dataset have one of two possible classes: benign (represented by 2) and malignant (represented by 4). Also, there are 10 attributes in this dataset (shown above) which will be used for prediction, except Sample Code Number which is the id number."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the data and rename the class values as 0/1 for model building (where 1 represents a malignant case). Also, let’s observe the distribution of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.drop(['Sample Code Number'],axis=1) #Drop 1st column\n",
    "data = data[data['Bare Nuclei'] != '?'] #Remove rows with missing data\n",
    "data['Class'] = np.where(data['Class'] ==2,0,1) #Change the Class representation\n",
    "data['Class'].value_counts() #Class distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before building a classification model, let’s build a Dummy Classifier to determine the ‘baseline’ performance. This answers the question — ‘What would be the success rate of the model, if one were simply guessing?’ The dummy classifier we are using will simply predict the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split data into attributes and class\n",
    "X = data.drop(['Class'],axis=1)\n",
    "y = data['Class']\n",
    "\n",
    "#perform training and test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "\n",
    "#Dummy Classifier\n",
    "from sklearn.dummy import DummyClassifier\n",
    "clf = DummyClassifier(strategy= 'most_frequent').fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "#Distribution of y test\n",
    "print('y actual : \\n' +  str(y_test.value_counts()))\n",
    "\n",
    "#Distribution of y predicted\n",
    "print('y predicted : \\n' + str(pd.Series(y_pred).value_counts()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the output, we can observe that there are 68 malignant and 103 benign cases in the test dataset. However, our classifier predicts all cases as benign (as it is the majority class)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Evaluation metrics \n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred)))\n",
    "\n",
    "#Dummy Classifier Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy of the model is 60.2%, but this is a case where accuracy may not be the best metric to evaluate the model. So, let’s take a look at the other evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression().fit(X_train,y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Model Evaluation metrics \n",
    "from sklearn.metrics import accuracy_score,recall_score,precision_score,f1_score\n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred)))\n",
    "\n",
    "#Logistic Regression Classifier Confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print('Confusion Matrix : \\n' + str(confusion_matrix(y_test,y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the misclassified instances, we can observe that 8 malignant cases have been classified incorrectly as benign (False negatives). Also, just one benign case has been classified as malignant (False positive).\n",
    "A false negative is more serious as a disease has been ignored, which can lead to the death of the patient. At the same time, a false positive would lead to an unnecessary treatment — incurring additional cost.\n",
    "Let’s try to minimize the false negatives by using Grid Search to find the optimal parameters. Grid search can be used to improve any specific evaluation metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "#Grid Search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "clf = LogisticRegression()\n",
    "pens = ['l1', 'l2']\n",
    "Cs = [0.009,0.01,.09,1,5,10,25]\n",
    "grid_values = {'penalty': pens,'C':Cs}\n",
    "grid_clf_acc = GridSearchCV(clf, param_grid = grid_values,scoring = 'recall')\n",
    "grid_clf_acc.fit(X_train, y_train)\n",
    "\n",
    "#Predict values based on new parameters\n",
    "y_pred_acc = grid_clf_acc.predict(X_test)\n",
    "\n",
    "# New Model Evaluation metrics \n",
    "print('Accuracy Score : ' + str(accuracy_score(y_test,y_pred_acc)))\n",
    "print('Precision Score : ' + str(precision_score(y_test,y_pred_acc)))\n",
    "print('Recall Score : ' + str(recall_score(y_test,y_pred_acc)))\n",
    "print('F1 Score : ' + str(f1_score(y_test,y_pred_acc)))\n",
    "\n",
    "#Logistic Regression (Grid Search) Confusion matrix\n",
    "confusion_matrix(y_test,y_pred_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hyperparameters we tuned are:\n",
    "1. Penalty: l1 or l2 which species the norm used in the penalization.\n",
    "2. C: Inverse of regularization strength- smaller values of C specify stronger regularization.\n",
    "\n",
    "Also, in Grid-search function, we have the scoring parameter where we can specify the metric to evaluate the model on (We have chosen recall as the metric). From the confusion matrix below, we can see that the number of false negatives has reduced, however, it is at the cost of increased false positives. The recall after grid search has jumped from 88.2% to 91.1%, whereas the precision has dropped to 87.3% from 98.3%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_acc.cv_results_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_acc.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_acc.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_clf_acc.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [x for x in grid_clf_acc.cv_results_['mean_train_score']]\n",
    "scores = np.array(scores).reshape(len(Cs), len(pens))\n",
    "\n",
    "for ind, i in enumerate(Cs):\n",
    "    plt.plot(pens, scores[ind], label='C: ' + str(i))\n",
    "plt.legend()\n",
    "plt.xlabel('Penalty')\n",
    "plt.ylabel('Mean score')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
